Test with barebone CNN

TESTS WITH DIFFERENT ACTIVATION FUNCTIONS

1st test: KeplerCNN
dataset = 50
batch = 128
lr = 1e-3
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.3
Other parameters = default

Final:
Best performance at epoch 78: {'accuracy': 0.5570776255707762, 'precision': 0.7536231884057971, 'recall': 0.3939393939393939, 'f1_score': 0.5174129353233831}

2nd test: KeplerCNN
dataset = 50
batch = 128
lr = 1e-3
activation = ReLU
patience = 3
factor = 0.5
dropout_rate = 0.3
Other parameters = default

Final:
Best performance at epoch 35: {'accuracy': 0.6027397260273972, 'precision': 0.6027397260273972, 'recall': 1.0, 'f1_score': 0.7521367521367521}

3rd test: KeplerCNN
dataset = 50
batch = 128
lr = 1e-3
activation = ELU
patience = 3
factor = 0.5
dropout_rate = 0.3
Other parameters = default

Final:
Best performance at epoch 1: {'accuracy': 0.6027397260273972, 'precision': 0.6027397260273972, 'recall': 1.0, 'f1_score': 0.7521367521367521}

TESTS WITH DROPOUT LAYERS
    From previous tests LeakyReLU leads to the most consistent results (metrics are 'near' to each other)
    other activations give excessively high recall and very low separability (measured with roc_auc)
    so we are now using LeakyReLU as our activation function

4th test: KeplerCNN
dataset = 50
batch = 128
lr = 1e-3
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.3
Other parameters = no dropout layer after the convolutional one, only before the linear layer

Final:
Best performance at epoch 6: {'accuracy': 0.593607305936073, 'precision': 0.6048780487804878, 'recall': 0.9393939393939394, 'f1_score': 0.7359050445103857}

5th test: KeplerCNN
dataset = 50
batch = 128
lr = 1e-3
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.3
Other parameters = no dropout layers

Final:
Best performance at epoch 13: {'accuracy': 0.5981735159817352, 'precision': 0.6078431372549019, 'recall': 0.9393939393939394, 'f1_score': 0.7380952380952381}

TEST WITH NORMALIZATION LAYERS
    From the previous tests we can see that it is better to keep the dropout layers

6th test: KeplerCNN
dataset = 50
batch = 128
lr = 1e-3
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.3
Other parameters = no normalization layers

Final:
Best performance at epoch 96: {'accuracy': 0.6027397260273972, 'precision': 0.6027397260273972, 'recall': 1.0, 'f1_score': 0.7521367521367521}

7th test: KeplerCNN
dataset = 50
batch = 128
lr = 1e-3
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.3
Other parameters = addition of another normalization layer + linear layer (with activation for added nonlinearity)

Final:
Best performance at epoch 96: {'accuracy': 0.6027397260273972, 'precision': 0.6027397260273972, 'recall': 1.0, 'f1_score': 0.7521367521367521}

8th test: KeplerCNN
dataset = 100 -> Bigger dataset might benefit more from the additional layer
batch = 128
lr = 1e-3
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.3
Other parameters = addition of another normalization layer + linear layer (with activation for added nonlinearity)

Final:
Best performance at epoch 6: {'accuracy': 0.6004618937644342, 'precision': 0.6004618937644342, 'recall': 1.0, 'f1_score': 0.7503607503607503}

TUNING DROPOUT RATE
    For previous tests, we mantain 2 dropout layers, 1 nomralization layer, LeakyReLU as activation and use a bigger dataset (now cached, makes running models faster)

9th test: KeplerCNN
dataset = 100 -> new run with the best model so far with bigger dataset to update metrics
batch = 128
lr = 1e-3
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.3
Other parameters = default

Final:
Best performance at epoch 40: {'accuracy': 0.6004618937644342, 'precision': 0.6004618937644342, 'recall': 1.0, 'f1_score': 0.7503607503607503}

10th test: KeplerCNN
dataset = 100
batch = 128
lr = 1e-3
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.5 
Other parameters = default

Final:
Best performance at epoch 21: {'accuracy': 0.581986143187067, 'precision': 0.6965174129353234, 'recall': 0.5384615384615384, 'f1_score': 0.6073752711496746}

11th test: KeplerCNN
dataset = 100
batch = 128
lr = 1e-3
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.7 
Other parameters = default

Final:
Best performance at epoch 27: {'accuracy': 0.6004618937644342, 'precision': 0.6004618937644342, 'recall': 1.0, 'f1_score': 0.7503607503607503}

12th test: KeplerCNN
dataset = 100
batch = 128
lr = 1e-3
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.6 
Other parameters = default

Final:
Best performance at epoch 2: {'accuracy': 0.42032332563510394, 'precision': 0.5141065830721003, 'recall': 0.6307692307692307, 'f1_score': 0.5664939550949913}


SCHEDULER TUNING
    From previou test, the dropout rate works well at 0.5

13th test: KeplerCNN
dataset = 100
batch = 128
lr = 1e-3
activation = LeakyReLU
factor = 0.5
dropout_rate = 0.5 
Scheduler: mode='min', factor=0.5, patience=2, threshold=1e-5, threshold_mode='rel', eps=1e-8

Final:
Best performance at epoch 44: {'accuracy': 0.6004618937644342, 'precision': 0.6004618937644342, 'recall': 1.0, 'f1_score': 0.7503607503607503}

TUNING LEARNING RATE
    Scheduler is kept at standard values, tuninig does not seem to improve performance

14th test: KeplerCNN
dataset = 100
batch = 128
lr = 5e-3 (from previous experience with AdamW seems to work well)
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.5 
Other parameters = default

Final:
Best performance at epoch 14: {'accuracy': 0.6004618937644342, 'precision': 0.6004618937644342, 'recall': 1.0, 'f1_score': 0.7503607503607503}

15th test: KeplerCNN
dataset = 100
batch = 128
lr = 3e-4 (also from previous experience with AdamW seems to work well)
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.5 
Other parameters = default

Final:
Best performance at epoch 32: {'accuracy': 0.6004618937644342, 'precision': 0.6004618937644342, 'recall': 1.0, 'f1_score': 0.7503607503607503}

16th test: KeplerCNN
dataset = 100
batch = 128
lr = 1e-5
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.5 
Other parameters = default

Final:
Best performance at epoch 68: {'accuracy': 0.6004618937644342, 'precision': 0.6004618937644342, 'recall': 1.0, 'f1_score': 0.7503607503607503}

17th test: KeplerCNN
dataset = 200 -> even bigger dataset (a bigger dataset would require too much time to download, this is the max)
batch = 128
lr = 1e-5
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.5 
Other parameters = default

Final:
Best performance at epoch 7: {'accuracy': 0.40239912758996726, 'precision': 0.510670731707317, 'recall': 0.5960854092526691, 'f1_score': 0.5500821018062397}

-> Bigger dataset is definitely the way to go (unfortunately starting to run into time and hardware restraints for testing)

18th test: KeplerCNN
dataset = 200 
batch = 128
lr = 5e-5 -> lower lr seems to lead to more consistent results together with a bigger dataset, testing this theory
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.5 
Other parameters = default

Final:
Best performance at epoch 1: {'accuracy': 0.6128680479825518, 'precision': 0.6128680479825518, 'recall': 1.0, 'f1_score': 0.7599729546991211}

-> lower learning rate did not help, going back to standard

19th test: KeplerCNN
dataset = 200 
batch = 128
lr = 1e-3 
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.5 
Other parameters = default

Final:
Epoch 100, Metrics: {'accuracy': 0.6106870229007634, 'precision': 0.8754578754578755, 'recall': 0.42526690391459077, 'f1_score': 0.5724550898203593}

-> Very good precision and recall does not explode to 1.0

20th test: KeplerCNN
dataset = 200 
batch = 128
lr = 5e-3 
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.5 
Other parameters = default

Final:
Best performance at epoch 10: {'accuracy': 0.6128680479825518, 'precision': 0.6128680479825518, 'recall': 1.0, 'f1_score': 0.7599729546991211}

-> Better f1 but the recall shoots up to 1.0 again, making results less consistent over the board

21st test: KeplerCNN
dataset = 200 
batch = 128
lr = 2.5e-3 
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.5 
Other parameters = default

Final:
Best performance at epoch 5: {'accuracy': 0.5768811341330425, 'precision': 0.8508064516129032, 'recall': 0.37544483985765126, 'f1_score': 0.5209876543209877}

LAST TEST: WITHOUT NOISE AUGMENTATION

22nd test: KeplerCNN
dataset = 200 
batch = 128
lr = 1e-3 
activation = LeakyReLU
patience = 3
factor = 0.5
dropout_rate = 0.5 
Other parameters = default

Final:
Best performance at epoch 17: {'accuracy': 0.6128680479825518, 'precision': 0.6128680479825518, 'recall': 1.0, 'f1_score': 0.7599729546991211}

-> Noise augmentation improves performance of the model